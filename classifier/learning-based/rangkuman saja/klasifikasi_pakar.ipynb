{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intelligent system               81\n",
      "software engineering             73\n",
      "sistem komputer                  70\n",
      "information management           67\n",
      "enterprise computing             52\n",
      "social and professional issue    38\n",
      "hardware                         27\n",
      "graphics and visualization       25\n",
      "algoritma dan pemrograman        22\n",
      "dasar matematika                  8\n",
      "Name: deskripsi, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation\n",
      "0.6559139784946236\n",
      "0.6559139784946236\n",
      "[0.375      0.         0.57142857 1.         0.83333333 0.57894737\n",
      " 0.625      0.73333333 1.         0.6       ]\n",
      "[1.         0.         0.72727273 0.33333333 0.83333333 0.6875\n",
      " 0.83333333 0.91666667 0.63636364 0.27272727]\n",
      "[[ 3  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  8  0  0  2  1  0  0  0]\n",
      " [ 1  0  1  3  1  2  0  1  0  0]\n",
      " [ 1  0  0  0  5  0  0  0  0  0]\n",
      " [ 0  0  2  0  0 11  2  0  0  1]\n",
      " [ 0  0  0  0  0  2 10  0  0  0]\n",
      " [ 0  0  1  0  0  0  0 11  0  0]\n",
      " [ 0  0  0  0  0  0  0  3  7  1]\n",
      " [ 2  0  2  0  0  2  2  0  0  3]]\n",
      "[0.54545455 0.         0.64       0.5        0.83333333 0.62857143\n",
      " 0.71428571 0.81481481 0.77777778 0.375     ]\n",
      "0.6559139784946236\n",
      "Same Data\n",
      "0.9891891891891892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import average_precision_score\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "#Initialize stop words for Bahasa Indonesia\n",
    "id_stop_words_file = pd.read_csv('id-stopwords.txt', header=None, names=['stopwords'])\n",
    "id_stop_words_list = []\n",
    "for i in range(len(id_stop_words_file)):\n",
    "    id_stop_words_list.append(id_stop_words_file.values[i][0])\n",
    "\n",
    "#Initialize Dataset\n",
    "dataset = pd.read_csv('rangkuman-saja.csv', encoding='latin', header=0, sep=',')\n",
    "input = dataset['judul_proposal'].str.lower().str.replace('[^a-zA-Z0-9 ]', '')\n",
    "target = dataset['deskripsi'].str.lower()\n",
    "\n",
    "#See dataset distribution\n",
    "print(dataset.deskripsi.value_counts())\n",
    "\n",
    "\n",
    "#Initialize Stemmer, Stemmernya cukup lama jadi comment aja biar cepet\n",
    "# factory = StemmerFactory()\n",
    "# stemmer = factory.create_stemmer()\n",
    "# for i in range(len(input)):\n",
    "#    print(i)\n",
    "#    input[i] = stemmer.stem(input[i])\n",
    "\n",
    "#Initialize TFIDF Vectorizer\n",
    "tvect = TfidfVectorizer(min_df=1,stop_words=id_stop_words_list)\n",
    "\n",
    "#Split Test dan Data Train\n",
    "x_train, x_test, y_train, y_test = train_test_split(input, target, test_size=0.2, random_state=4)\n",
    "\n",
    "\n",
    "\n",
    "tvect1 = TfidfVectorizer(min_df=1,stop_words=id_stop_words_list)\n",
    "#tvect1 = CountVectorizer()\n",
    "x_traincv=tvect1.fit_transform(x_train)\n",
    "x_testcv=tvect1.transform(x_test)\n",
    "x_traincv=tvect1.fit_transform(x_train)\n",
    "x_testcv=tvect1.transform(x_test)\n",
    "\n",
    "#Oversampling data use SMOTE\n",
    "sm = SMOTE(random_state=4, ratio=1)\n",
    "# sm = ADASYN()\n",
    "x_train_res, y_train_res = sm.fit_sample(x_traincv, y_train)\n",
    "\n",
    "# model = RandomForestClassifier(n_estimators=50) #around 54%\n",
    "model = LinearSVC() #around 67%\n",
    "# model = LinearSVC(loss='hinge', multi_class='ovr') # 51%\n",
    "\n",
    "# y_train = y_train_res.astype('str')\n",
    "# model.fit(x_traincv,y_train)\n",
    "\n",
    "y_train_res = y_train_res.astype('str')\n",
    "model.fit(x_train_res,y_train_res)\n",
    "\n",
    "print(\"Cross Validation\")\n",
    "predictions=model.predict(x_testcv)\n",
    "count = 0\n",
    "for i in range (len(predictions)):\n",
    "  if predictions[i]==y_test.values[i]:\n",
    "     count = count + 1\n",
    "print(count / len(y_test))\n",
    "\n",
    "#Evaluation Metrics\n",
    "print(accuracy_score(y_test, predictions))\n",
    "print(precision_score(y_test, predictions, average=None))\n",
    "print(recall_score(y_test, predictions, average=None))  \n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(f1_score(y_test, predictions, average=None))\n",
    "print(f1_score(y_test, predictions, average='micro'))\n",
    "\n",
    "print(\"Same Data\")\n",
    "predictions=model.predict(x_traincv)\n",
    "count = 0\n",
    "for i in range (len(predictions)):\n",
    "  # print(predictions[i])\n",
    "  # print(y_train.values[i])\n",
    "  if predictions[i]==y_train.values[i]:\n",
    "     count = count + 1\n",
    "print(count / len(y_train))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#SAVE MODEL\n",
    "# save the model to disk\n",
    "filename = 'KlasifikasiPakar.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation\n",
      "0.9435897435897436\n",
      "Same Data\n",
      "0.9948717948717949\n",
      "['social and professional issue']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import average_precision_score\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "\n",
    "#Initialize stop words for Bahasa Indonesia\n",
    "id_stop_words_file = pd.read_csv('id-stopwords.txt', header=None, names=['stopwords'])\n",
    "id_stop_words_list = []\n",
    "for i in range(len(id_stop_words_file)):\n",
    "    id_stop_words_list.append(id_stop_words_file.values[i][0])\n",
    "\n",
    "#Initialize Dataset\n",
    "dataset = pd.read_csv('rangkuman-saja-copy.csv', header=0, sep=',', encoding='latin')\n",
    "input = dataset['judul_proposal'].str.lower().str.replace('[^a-zA-Z0-9 ]', '')\n",
    "target = dataset['deskripsi'].str.lower()\n",
    "\n",
    "#Initialize Stemmer, Stemmernya cukup lama jadi comment aja biar cepet\n",
    "#factory = StemmerFactory()\n",
    "#stemmer = factory.create_stemmer()\n",
    "#for i in range(len(input)):\n",
    "#    print i\n",
    "#    input[i] = stemmer.stem(input[i])\n",
    "\n",
    "#Initialize TFIDF Vectorizer\n",
    "tvect = TfidfVectorizer(min_df=1,stop_words=id_stop_words_list)\n",
    "\n",
    "#Split Test dan Data Train\n",
    "x_train, x_test, y_train, y_test = train_test_split(input, target, test_size=0.2, random_state=4)\n",
    "\n",
    "tvect1 = TfidfVectorizer(min_df=1,stop_words=id_stop_words_list)\n",
    "x_traincv=tvect1.fit_transform(x_train)\n",
    "x_testcv=tvect1.transform(x_test)\n",
    "x_traincv=tvect1.fit_transform(x_train)\n",
    "x_testcv=tvect1.transform(x_test)\n",
    "\n",
    "\n",
    "filename = 'KlasifikasiPakar.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "y_train = y_train.astype('str')\n",
    "loaded_model.fit(x_traincv,y_train)\n",
    "\n",
    "print(\"Cross Validation\")\n",
    "predictions=loaded_model.predict(x_testcv)\n",
    "count = 0\n",
    "for i in range (len(predictions)):\n",
    "  if predictions[i]==y_test.values[i]:\n",
    "     count = count + 1\n",
    "print(count / len(y_test))\n",
    "\n",
    "\n",
    "print(\"Same Data\")\n",
    "predictions=loaded_model.predict(x_traincv)\n",
    "count = 0\n",
    "for i in range (len(predictions)):\n",
    "  if predictions[i]==y_train.values[i]:\n",
    "     count = count + 1\n",
    "print(count / len(y_train))\n",
    "\n",
    "# Contoh 1 data\n",
    "x = ['']\n",
    "x_new=tvect1.transform(x)\n",
    "new_pred = loaded_model.predict(x_new)\n",
    "print(new_pred)\n",
    "\n",
    "filename = 'tvect.sav'\n",
    "pickle.dump(tvect1, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
