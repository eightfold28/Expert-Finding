{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "information management           135\n",
      "intelligent system               129\n",
      "software engineering             128\n",
      "sistem komputer                  123\n",
      "enterprise computing             118\n",
      "social and professional issue     52\n",
      "graphics and visualization        42\n",
      "hardware                          41\n",
      "algoritma dan pemrograman         22\n",
      "dasar matematika                   8\n",
      "Name: deskripsi, dtype: int64\n",
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=['ada', 'adalah', 'adanya', 'adapun', 'agak', 'agaknya', 'agar', 'akan', 'akankah', 'akhir', 'akhiri', 'akhirnya', 'aku', 'akulah', 'amat', 'amatlah', 'anda', 'andalah', 'antar', 'antara', 'antaranya', 'apa', 'apaan', 'apabila', 'apakah', 'apalagi', 'apatah', 'artinya', 'asal', 'asalkan',...'wah', 'wahai', 'waktu', 'waktunya', 'walau', 'walaupun', 'wong', 'yaitu', 'yakin', 'yakni', 'yang'],\n",
      "        strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "start\n",
      "software engineering             31\n",
      "enterprise computing             31\n",
      "information management           26\n",
      "intelligent system               25\n",
      "sistem komputer                  20\n",
      "social and professional issue    10\n",
      "graphics and visualization        8\n",
      "hardware                          8\n",
      "algoritma dan pemrograman         1\n",
      "Name: deskripsi, dtype: int64\n",
      "end\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation\n",
      "0.76875\n",
      "0.76875\n",
      "0.76875\n",
      "[1.         0.80645161 0.5        0.625      0.92307692 0.88\n",
      " 0.55       0.8        0.74193548]\n",
      "[[ 1  0  0  0  0  0  0  0  0]\n",
      " [ 0 25  0  0  1  2  0  1  2]\n",
      " [ 0  0  4  0  1  2  0  0  1]\n",
      " [ 0  0  0  5  0  3  0  0  0]\n",
      " [ 0  0  0  0 24  0  0  0  2]\n",
      " [ 0  1  0  0  1 22  1  0  0]\n",
      " [ 0  0  1  1  3  4 11  0  0]\n",
      " [ 0  1  0  0  1  0  0  8  0]\n",
      " [ 0  4  0  1  0  0  0  3 23]]\n",
      "[1.         0.80645161 0.61538462 0.66666667 0.84210526 0.75862069\n",
      " 0.6875     0.72727273 0.77966102]\n",
      "0.76875\n",
      "Same Data\n",
      "0.9639498432601881\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import average_precision_score\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "#Initialize stop words for Bahasa Indonesia\n",
    "id_stop_words_file = pd.read_csv('id-stopwords.txt', header=None, names=['stopwords'])\n",
    "id_stop_words_list = []\n",
    "for i in range(len(id_stop_words_file)):\n",
    "    id_stop_words_list.append(id_stop_words_file.values[i][0])\n",
    "\n",
    "#Initialize Dataset\n",
    "dataset = pd.read_csv('data-judul-saja.csv', encoding='latin', header=0, sep=',')\n",
    "input = dataset['judul_proposal'].str.lower().str.replace('[^a-zA-Z0-9 ]', '')\n",
    "target = dataset['deskripsi'].str.lower()\n",
    "\n",
    "#See dataset distribution\n",
    "print(dataset.deskripsi.value_counts())\n",
    "\n",
    "\n",
    "#Initialize Stemmer, Stemmernya cukup lama jadi comment aja biar cepet\n",
    "# factory = StemmerFactory()\n",
    "# stemmer = factory.create_stemmer()\n",
    "# for i in range(len(input)):\n",
    "#    print(i)\n",
    "#    input[i] = stemmer.stem(input[i])\n",
    "\n",
    "#Initialize TFIDF Vectorizer\n",
    "tvect = TfidfVectorizer(min_df=1,stop_words=id_stop_words_list)\n",
    "print(tvect)\n",
    "\n",
    "#Split Test dan Data Train\n",
    "x_train, x_test, y_train, y_test = train_test_split(input, target, test_size=0.2, random_state=4)\n",
    "\n",
    "print(\"start\")\n",
    "print(y_test.value_counts())\n",
    "print(\"end\")\n",
    "\n",
    "tvect1 = TfidfVectorizer(min_df=1,stop_words=id_stop_words_list)\n",
    "\n",
    "x_traincv=tvect1.fit_transform(x_train)\n",
    "x_testcv=tvect1.transform(x_test)\n",
    "x_traincv=tvect1.fit_transform(x_train)\n",
    "x_testcv=tvect1.transform(x_test)\n",
    "\n",
    "#Oversampling data use SMOTE\n",
    "sm = SMOTE(random_state=4, ratio=1)\n",
    "# sm = ADASYN()\n",
    "x_train_res, y_train_res = sm.fit_sample(x_traincv, y_train)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=5) #around 54%\n",
    "# model = LinearSVC() #around 67%\n",
    "# model = LinearSVC(loss='hinge', multi_class='ovr') # 51%\n",
    "\n",
    "# y_train = y_train_res.astype('str')\n",
    "# model.fit(x_traincv,y_train)\n",
    "\n",
    "y_train_res = y_train_res.astype('str')\n",
    "model.fit(x_train_res,y_train_res)\n",
    "\n",
    "print(\"Cross Validation\")\n",
    "predictions=model.predict(x_testcv)\n",
    "count = 0\n",
    "for i in range (len(predictions)):\n",
    "  if predictions[i]==y_test.values[i]:\n",
    "     count = count + 1\n",
    "print(count / len(y_test))\n",
    "\n",
    "#Evaluation Metrics\n",
    "print(accuracy_score(y_test, predictions))\n",
    "print(precision_score(y_test, predictions, average='micro'))\n",
    "print(recall_score(y_test, predictions, average=None))  \n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(f1_score(y_test, predictions, average=None))\n",
    "print(f1_score(y_test, predictions, average='micro'))\n",
    "\n",
    "print(\"Same Data\")\n",
    "predictions=model.predict(x_traincv)\n",
    "count = 0\n",
    "for i in range (len(predictions)):\n",
    "  # print(predictions[i])\n",
    "  # print(y_train.values[i])\n",
    "  if predictions[i]==y_train.values[i]:\n",
    "     count = count + 1\n",
    "print(count / len(y_train))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#SAVE MODEL\n",
    "# save the model to disk\n",
    "filename = 'KlasifikasiPakar.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation\n",
      "0.7359550561797753\n",
      "Same Data\n",
      "0.9647887323943662\n",
      "['intelligent system']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import average_precision_score\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "\n",
    "#Initialize stop words for Bahasa Indonesia\n",
    "id_stop_words_file = pd.read_csv('id-stopwords.txt', header=None, names=['stopwords'])\n",
    "id_stop_words_list = []\n",
    "for i in range(len(id_stop_words_file)):\n",
    "    id_stop_words_list.append(id_stop_words_file.values[i][0])\n",
    "\n",
    "#Initialize Dataset\n",
    "dataset = pd.read_csv('data-judul-saja-silabus.csv', encoding='latin', header=0, sep=',')\n",
    "input = dataset['judul_proposal'].str.lower().str.replace('[^a-zA-Z0-9 ]', '')\n",
    "target = dataset['deskripsi'].str.lower()\n",
    "\n",
    "#Initialize Stemmer, Stemmernya cukup lama jadi comment aja biar cepet\n",
    "#factory = StemmerFactory()\n",
    "#stemmer = factory.create_stemmer()\n",
    "#for i in range(len(input)):\n",
    "#    print i\n",
    "#    input[i] = stemmer.stem(input[i])\n",
    "\n",
    "#Initialize TFIDF Vectorizer\n",
    "tvect = TfidfVectorizer(min_df=1,stop_words=id_stop_words_list)\n",
    "\n",
    "#Split Test dan Data Train\n",
    "x_train, x_test, y_train, y_test = train_test_split(input, target, test_size=0.2, random_state=4)\n",
    "\n",
    "tvect1 = TfidfVectorizer(min_df=1,stop_words=id_stop_words_list)\n",
    "x_traincv=tvect1.fit_transform(x_train)\n",
    "x_testcv=tvect1.transform(x_test)\n",
    "x_traincv=tvect1.fit_transform(x_train)\n",
    "x_testcv=tvect1.transform(x_test)\n",
    "\n",
    "\n",
    "filename = 'KlasifikasiPakar.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "y_train = y_train.astype('str')\n",
    "loaded_model.fit(x_traincv,y_train)\n",
    "\n",
    "print(\"Cross Validation\")\n",
    "predictions=loaded_model.predict(x_testcv)\n",
    "count = 0\n",
    "for i in range (len(predictions)):\n",
    "  if predictions[i]==y_test.values[i]:\n",
    "     count = count + 1\n",
    "print(count / len(y_test))\n",
    "\n",
    "\n",
    "print(\"Same Data\")\n",
    "predictions=loaded_model.predict(x_traincv)\n",
    "count = 0\n",
    "for i in range (len(predictions)):\n",
    "  if predictions[i]==y_train.values[i]:\n",
    "     count = count + 1\n",
    "print(count / len(y_train))\n",
    "\n",
    "# Contoh 1 data\n",
    "# x = ['aplikasi cloud di kota jakarta']\n",
    "x = ['Kuliah ini membahas berbagai teknik pembelajaran oleh mesin serta pengenalan pola. Pendekatan yang diberikan mencakup supervised learning, unsupervised learning, dan reinforcement learning.']\n",
    "x_new=tvect1.transform(x)\n",
    "new_pred = loaded_model.predict(x_new)\n",
    "print(new_pred)\n",
    "\n",
    "filename = 'tvect.sav'\n",
    "pickle.dump(tvect1, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
