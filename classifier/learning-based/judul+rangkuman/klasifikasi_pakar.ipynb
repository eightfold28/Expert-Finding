{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enterprise computing             93\n",
      "information management           89\n",
      "intelligent system               84\n",
      "software engineering             82\n",
      "sistem komputer                  78\n",
      "hardware                         40\n",
      "social and professional issue    40\n",
      "graphics and visualization       26\n",
      "algoritma dan pemrograman        14\n",
      "dasar matematika                  8\n",
      "Name: deskripsi, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation\n",
      "0.7387387387387387\n",
      "0.7387387387387387\n",
      "[0.2        0.         0.80952381 1.         1.         0.59090909\n",
      " 0.66666667 0.875      0.875      0.73333333]\n",
      "[0.25       0.         0.80952381 0.5        0.88888889 0.68421053\n",
      " 0.76923077 0.875      1.         0.61111111]\n",
      "[[ 1  0  0  0  0  0  0  0  0  3]\n",
      " [ 1  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0 17  0  0  3  0  1  0  0]\n",
      " [ 0  0  0  1  0  0  0  1  0  0]\n",
      " [ 1  0  0  0  8  0  0  0  0  0]\n",
      " [ 0  0  3  0  0 13  2  0  0  1]\n",
      " [ 0  0  0  0  0  3 10  0  0  0]\n",
      " [ 0  0  1  0  0  1  0 14  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  7  0]\n",
      " [ 2  0  0  0  0  2  2  0  1 11]]\n",
      "[0.22222222 0.         0.80952381 0.66666667 0.94117647 0.63414634\n",
      " 0.71428571 0.875      0.93333333 0.66666667]\n",
      "0.7387387387387387\n",
      "Same Data\n",
      "0.9932279909706546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import average_precision_score\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "#Initialize stop words for Bahasa Indonesia\n",
    "id_stop_words_file = pd.read_csv('id-stopwords.txt', header=None, names=['stopwords'])\n",
    "id_stop_words_list = []\n",
    "for i in range(len(id_stop_words_file)):\n",
    "    id_stop_words_list.append(id_stop_words_file.values[i][0])\n",
    "\n",
    "#Initialize Dataset\n",
    "dataset = pd.read_csv('judul-dan-rangkuman.csv', encoding='latin', header=0, sep=',')\n",
    "input = dataset['judul_proposal'].str.lower().str.replace('[^a-zA-Z0-9 ]', '')\n",
    "target = dataset['deskripsi'].str.lower()\n",
    "\n",
    "#See dataset distribution\n",
    "print(dataset.deskripsi.value_counts())\n",
    "\n",
    "\n",
    "#Initialize Stemmer, Stemmernya cukup lama jadi comment aja biar cepet\n",
    "# factory = StemmerFactory()\n",
    "# stemmer = factory.create_stemmer()\n",
    "# for i in range(len(input)):\n",
    "#    print(i)\n",
    "#    input[i] = stemmer.stem(input[i])\n",
    "\n",
    "#Initialize TFIDF Vectorizer\n",
    "tvect = TfidfVectorizer(min_df=1,stop_words=id_stop_words_list)\n",
    "\n",
    "#Split Test dan Data Train\n",
    "x_train, x_test, y_train, y_test = train_test_split(input, target, test_size=0.2, random_state=4)\n",
    "\n",
    "\n",
    "\n",
    "tvect1 = TfidfVectorizer(min_df=1,stop_words=id_stop_words_list)\n",
    "#tvect1 = CountVectorizer()\n",
    "x_traincv=tvect1.fit_transform(x_train)\n",
    "x_testcv=tvect1.transform(x_test)\n",
    "x_traincv=tvect1.fit_transform(x_train)\n",
    "x_testcv=tvect1.transform(x_test)\n",
    "\n",
    "#Oversampling data use SMOTE\n",
    "sm = SMOTE(random_state=4, ratio=1)\n",
    "# sm = ADASYN()\n",
    "x_train_res, y_train_res = sm.fit_sample(x_traincv, y_train)\n",
    "\n",
    "# model = RandomForestClassifier(n_estimators=50) #around 54%\n",
    "model = LinearSVC() #around 67%\n",
    "# model = LinearSVC(loss='hinge', multi_class='ovr') # 51%\n",
    "\n",
    "# y_train = y_train_res.astype('str')\n",
    "# model.fit(x_traincv,y_train)\n",
    "\n",
    "y_train_res = y_train_res.astype('str')\n",
    "model.fit(x_train_res,y_train_res)\n",
    "\n",
    "print(\"Cross Validation\")\n",
    "predictions=model.predict(x_testcv)\n",
    "count = 0\n",
    "for i in range (len(predictions)):\n",
    "  if predictions[i]==y_test.values[i]:\n",
    "     count = count + 1\n",
    "print(count / len(y_test))\n",
    "\n",
    "#Evaluation Metrics\n",
    "print(accuracy_score(y_test, predictions))\n",
    "print(precision_score(y_test, predictions, average=None))\n",
    "print(recall_score(y_test, predictions, average=None))  \n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(f1_score(y_test, predictions, average=None))\n",
    "print(f1_score(y_test, predictions, average='micro'))\n",
    "\n",
    "print(\"Same Data\")\n",
    "predictions=model.predict(x_traincv)\n",
    "count = 0\n",
    "for i in range (len(predictions)):\n",
    "  # print(predictions[i])\n",
    "  # print(y_train.values[i])\n",
    "  if predictions[i]==y_train.values[i]:\n",
    "     count = count + 1\n",
    "print(count / len(y_train))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#SAVE MODEL\n",
    "# save the model to disk\n",
    "filename = 'KlasifikasiPakar.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation\n",
      "0.4125874125874126\n",
      "Same Data\n",
      "1.0\n",
      "['sistem komputer']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import average_precision_score\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "\n",
    "#Initialize stop words for Bahasa Indonesia\n",
    "id_stop_words_file = pd.read_csv('id-stopwords.txt', header=None, names=['stopwords'])\n",
    "id_stop_words_list = []\n",
    "for i in range(len(id_stop_words_file)):\n",
    "    id_stop_words_list.append(id_stop_words_file.values[i][0])\n",
    "\n",
    "#Initialize Dataset\n",
    "dataset = pd.read_csv('judul-dan-rangkuman.csv', encoding='latin', header=0, sep=',')\n",
    "input = dataset['judul_proposal'].str.lower().str.replace('[^a-zA-Z0-9 ]', '')\n",
    "target = dataset['deskripsi'].str.lower()\n",
    "\n",
    "#Initialize Stemmer, Stemmernya cukup lama jadi comment aja biar cepet\n",
    "#factory = StemmerFactory()\n",
    "#stemmer = factory.create_stemmer()\n",
    "#for i in range(len(input)):\n",
    "#    print i\n",
    "#    input[i] = stemmer.stem(input[i])\n",
    "\n",
    "#Initialize TFIDF Vectorizer\n",
    "tvect = TfidfVectorizer(min_df=1,stop_words=id_stop_words_list)\n",
    "\n",
    "#Split Test dan Data Train\n",
    "x_train, x_test, y_train, y_test = train_test_split(input, target, test_size=0.2, random_state=4)\n",
    "\n",
    "tvect1 = TfidfVectorizer(min_df=1,stop_words=id_stop_words_list)\n",
    "x_traincv=tvect1.fit_transform(x_train.values.astype('U'))\n",
    "x_testcv=tvect1.transform(x_test.values.astype('U'))\n",
    "x_traincv=tvect1.fit_transform(x_train.values.astype('U'))\n",
    "x_testcv=tvect1.transform(x_test.values.astype('U'))\n",
    "\n",
    "\n",
    "filename = 'KlasifikasiPakar.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "y_train = y_train.astype('str')\n",
    "loaded_model.fit(x_traincv,y_train)\n",
    "\n",
    "print(\"Cross Validation\")\n",
    "predictions=loaded_model.predict(x_testcv)\n",
    "count = 0\n",
    "for i in range (len(predictions)):\n",
    "  if predictions[i]==y_test.values[i]:\n",
    "     count = count + 1\n",
    "print(count / len(y_test))\n",
    "\n",
    "\n",
    "print(\"Same Data\")\n",
    "predictions=loaded_model.predict(x_traincv)\n",
    "count = 0\n",
    "for i in range (len(predictions)):\n",
    "  if predictions[i]==y_train.values[i]:\n",
    "     count = count + 1\n",
    "print(count / len(y_train))\n",
    "\n",
    "# Contoh 1 data\n",
    "x = ['aplikasi cloud di kota jakarta']\n",
    "x_new=tvect1.transform(x)\n",
    "new_pred = loaded_model.predict(x_new)\n",
    "print(new_pred)\n",
    "\n",
    "filename = 'tvect.sav'\n",
    "pickle.dump(tvect1, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
